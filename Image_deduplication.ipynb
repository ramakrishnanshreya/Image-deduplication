{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19756c1-f251-4b15-b343-5aebd159e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Using cached scikit_image-0.24.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\python312\\lib\\site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\python312\\lib\\site-packages (from scikit-image) (1.14.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\python312\\lib\\site-packages (from scikit-image) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\python312\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\python312\\lib\\site-packages (from scikit-image) (2.36.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\python312\\lib\\site-packages (from scikit-image) (2024.9.20)\n",
      "Requirement already satisfied: packaging>=21 in c:\\python312\\lib\\site-packages (from scikit-image) (24.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\python312\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Using cached scikit_image-0.24.0-cp312-cp312-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: scikit-image\n",
      "Successfully installed scikit-image-0.24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4fbfeb-8af6-42ff-9b77-62e2d39a428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C50F701DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/python-matplotlib/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C5105E7E60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/python-matplotlib/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C510E389E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/python-matplotlib/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C510E38BC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/python-matplotlib/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C510E38DD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/python-matplotlib/\n",
      "ERROR: Could not find a version that satisfies the requirement python-matplotlib (from versions: none)\n",
      "ERROR: No matching distribution found for python-matplotlib\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-matplotlib python-numpy python-pil python-scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4459e9e8-23bd-4ab7-bb2b-6a861f0b0cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWG1 REG2 H1038 I1608 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H1038 I1608 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H12048 I14172 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H12048 I14172 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H2215 I3206 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H2215 I3206 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H2217 I3209 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H2217 I3209 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H23227 I26340 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H23227 I26340 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H23228 I26341 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H23228 I26341 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H23230 I26343 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H23230 I26343 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H23864 I27015 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H23864 I27015 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H25662 I28890 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H25662 I28890 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H25667 I28896 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H25667 I28896 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H25669 I28898 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H25669 I28898 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H2616 I3689 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H2616 I3689 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H2617 I3690 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H2617 I3690 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H2618 I3691 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H2618 I3691 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H2911 I4042 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H2911 I4042 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H2912 I4043 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H2912 I4043 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H29646 I33000 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H29646 I33000 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H32126 I35546 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H32126 I35546 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H32128 I35548 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H32128 I35548 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H32130 I35550 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H32130 I35550 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H3236 I4398 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H3236 I4398 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H3237 I4399 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H3237 I4399 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H3239 I4401 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H3239 I4401 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H3240 I4402 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H3240 I4402 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H33516 I36948 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H33516 I36948 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H4088 I5345 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H4088 I5345 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H5170 I6559 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H5170 I6559 H&N1  Q1194.jpg: 3 times\n",
      "NWG1 REG2 H5174 I6563 H&N1  Q1194(1).jpg: 3 times\n",
      "NWG1 REG2 H5174 I6563 H&N1  Q1194.jpg: 3 times\n",
      "\n",
      "Total unique images that are duplicates: 56\n",
      "Moved 28 duplicate images to 'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\Repeated Images'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def load_and_downsample(image_path, scale=0.5):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is not None:\n",
    "        height, width = image.shape\n",
    "        image = cv2.resize(image, (int(width * scale), int(height * scale)))\n",
    "    return image\n",
    "\n",
    "def compare_images(imgA, imgB):\n",
    "    # Compute SSIM between two images\n",
    "    return ssim(imgA, imgB)\n",
    "\n",
    "def find_duplicate_counts_and_move(image_folder, duplicates_folder):\n",
    "    os.makedirs(duplicates_folder, exist_ok=True)\n",
    "\n",
    "    descriptors = {}\n",
    "    images = {}\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith(('png', 'jpg', 'jpeg'))]\n",
    "    \n",
    "    # Using both SIFT and ORB\n",
    "    sift = cv2.SIFT_create()\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        image = load_and_downsample(img_path, scale=0.5)\n",
    "\n",
    "        if image is None:\n",
    "            continue\n",
    "        \n",
    "        images[img_file] = image\n",
    "        \n",
    "        # SIFT descriptor\n",
    "        keypoints_sift, desc_sift = sift.detectAndCompute(image, None)\n",
    "        if desc_sift is not None and len(desc_sift) > 0:\n",
    "            descriptors[img_file] = {'sift': desc_sift}\n",
    "\n",
    "        # ORB descriptor\n",
    "        keypoints_orb, desc_orb = orb.detectAndCompute(image, None)\n",
    "        if desc_orb is not None and len(desc_orb) > 0:\n",
    "            descriptors[img_file]['orb'] = desc_orb\n",
    "\n",
    "    duplicate_counts = defaultdict(int)\n",
    "    duplicate_images = set()\n",
    "\n",
    "    for i in range(len(image_files)):\n",
    "        for j in range(i + 1, len(image_files)):\n",
    "            if image_files[i] not in descriptors or image_files[j] not in descriptors:\n",
    "                continue\n",
    "            \n",
    "            # Compare using SIFT\n",
    "            if 'sift' in descriptors[image_files[i]] and 'sift' in descriptors[image_files[j]]:\n",
    "                bf_sift = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "                matches_sift = bf_sift.match(descriptors[image_files[i]]['sift'], descriptors[image_files[j]]['sift'])\n",
    "                good_matches_sift = [m for m in matches_sift if m.distance < 50]\n",
    "                \n",
    "                # Compare using SSIM if good matches are found\n",
    "                if len(good_matches_sift) > 0.1 * len(descriptors[image_files[i]]['sift']):\n",
    "                    imgA = images[image_files[i]]\n",
    "                    imgB = images[image_files[j]]\n",
    "                    similarity_index = compare_images(imgA, imgB)\n",
    "                    \n",
    "                    if similarity_index > 0.7:  # Adjust this threshold as needed\n",
    "                        duplicate_counts[image_files[i]] += 1\n",
    "                        duplicate_counts[image_files[j]] += 1\n",
    "                        duplicate_images.add(image_files[j])\n",
    "            \n",
    "            # Compare using ORB if SIFT fails\n",
    "            if 'orb' in descriptors[image_files[i]] and 'orb' in descriptors[image_files[j]]:\n",
    "                bf_orb = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "                matches_orb = bf_orb.match(descriptors[image_files[i]]['orb'], descriptors[image_files[j]]['orb'])\n",
    "                good_matches_orb = [m for m in matches_orb if m.distance < 30]\n",
    "\n",
    "                if len(good_matches_orb) > 0.1 * len(descriptors[image_files[i]]['orb']):\n",
    "                    imgA = images[image_files[i]]\n",
    "                    imgB = images[image_files[j]]\n",
    "                    similarity_index = compare_images(imgA, imgB)\n",
    "\n",
    "                    if similarity_index > 0.7:  # Adjust this threshold as needed\n",
    "                        duplicate_counts[image_files[i]] += 1\n",
    "                        duplicate_counts[image_files[j]] += 1\n",
    "                        duplicate_images.add(image_files[j])\n",
    "\n",
    "    total_duplicates = {img: count + 1 for img, count in duplicate_counts.items() if count > 0}\n",
    "    \n",
    "    for img, count in total_duplicates.items():\n",
    "        print(f\"{img}: {count} times\")\n",
    "\n",
    "    for img in duplicate_images:\n",
    "        shutil.move(os.path.join(image_folder, img), os.path.join(duplicates_folder, img))\n",
    "\n",
    "    print(f\"\\nTotal unique images that are duplicates: {len(total_duplicates)}\")\n",
    "    print(f\"Moved {len(duplicate_images)} duplicate images to '{duplicates_folder}'.\")\n",
    "\n",
    "# Usage\n",
    "image_folder = r'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\ImagesofFace'\n",
    "duplicates_folder = r'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\Repeated Images'\n",
    "find_duplicate_counts_and_move(image_folder, duplicates_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53ac277-0fe3-4b23-a456-d8e555af8cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected in NWG1 REG2 H1038 I1608 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H1038 I1608 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H1038 I1608 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H12048 I14172 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H12048 I14172 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H12048 I14172 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H2215 I3206 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H2215 I3206 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H2215 I3206 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H2217 I3209 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H2217 I3209 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H2217 I3209 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H23227 I26340 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H23227 I26340 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H23227 I26340 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H23228 I26341 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H23228 I26341 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H23228 I26341 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H23230 I26343 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H23864 I27015 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H23864 I27015 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H23864 I27015 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H25662 I28890 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H25662 I28890 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H25662 I28890 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H25667 I28896 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H25667 I28896 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H25667 I28896 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H25669 I28898 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H25669 I28898 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H25669 I28898 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H2616 I3689 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H2616 I3689 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H2616 I3689 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H2617 I3690 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H2617 I3690 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H2617 I3690 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H2618 I3691 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H2618 I3691 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H2618 I3691 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H2911 I4042 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H2911 I4042 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H2911 I4042 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H2912 I4043 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H2912 I4043 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H2912 I4043 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H29646 I33000 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H29646 I33000 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H29646 I33000 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H32126 I35546 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H32126 I35546 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H32126 I35546 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H32128 I35548 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H32128 I35548 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H32128 I35548 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H32130 I35550 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H32130 I35550 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H32130 I35550 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H3236 I4398 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H3236 I4398 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H3236 I4398 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H3237 I4399 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H3237 I4399 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H3237 I4399 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H3239 I4401 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H3239 I4401 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H3240 I4402 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H3240 I4402 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H3240 I4402 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H33516 I36948 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H33516 I36948 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H4088 I5345 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H4088 I5345 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H4088 I5345 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H5170 I6559 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H5170 I6559 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H5170 I6559 H&N1  Q1194.jpg\n",
      "No face detected in NWG1 REG2 H5174 I6563 H&N1  Q1194(1).jpg\n",
      "No face detected in NWG1 REG2 H5174 I6563 H&N1  Q1194(2).jpg\n",
      "No face detected in NWG1 REG2 H5174 I6563 H&N1  Q1194.jpg\n",
      "Moved 2 duplicate images to 'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\Repeated Images'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "def find_duplicates(image_folder, duplicates_folder, threshold=0.6):\n",
    "    os.makedirs(duplicates_folder, exist_ok=True)\n",
    "    embeddings = {}\n",
    "    duplicate_images = set()\n",
    "\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        image = face_recognition.load_image_file(img_path)\n",
    "\n",
    "        # Get face encodings for the image\n",
    "        face_encodings = face_recognition.face_encodings(image)\n",
    "        if not face_encodings:\n",
    "            print(f\"No face detected in {img_file}\")\n",
    "            continue\n",
    "\n",
    "        encoding = face_encodings[0]  # Get the first face encoding\n",
    "\n",
    "        # Compare with existing embeddings to find duplicates\n",
    "        is_duplicate = False\n",
    "        for existing_file, existing_encoding in embeddings.items():\n",
    "            distance = face_recognition.face_distance([existing_encoding], encoding)[0]\n",
    "            if distance < threshold:\n",
    "                duplicate_images.add(img_file)\n",
    "                is_duplicate = True\n",
    "                break\n",
    "\n",
    "        if not is_duplicate:\n",
    "            embeddings[img_file] = encoding\n",
    "\n",
    "    # Move duplicate images to duplicates folder\n",
    "    for img in duplicate_images:\n",
    "        shutil.move(os.path.join(image_folder, img), os.path.join(duplicates_folder, img))\n",
    "\n",
    "    print(f\"Moved {len(duplicate_images)} duplicate images to '{duplicates_folder}'.\")\n",
    "\n",
    "# Usage\n",
    "image_folder = r'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\ImagesofFace'\n",
    "duplicates_folder = r'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\Repeated Images'\n",
    "find_duplicates(image_folder, duplicates_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9d4d73-e43c-4356-93d3-e07eae24c04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 29 duplicate images to 'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\Repeated Images'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "def orb_similarity(img1, img2, threshold=30):\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "    \n",
    "    # Find keypoints and descriptors\n",
    "    kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "    kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "    \n",
    "    if des1 is None or des2 is None:\n",
    "        return False  # No features detected, not similar\n",
    "\n",
    "    # Match descriptors using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    # Count matches\n",
    "    similar_regions = [m for m in matches if m.distance < threshold]\n",
    "    return len(similar_regions) > 15  # Adjust this number based on similarity tolerance\n",
    "\n",
    "def find_duplicates(image_folder, duplicates_folder):\n",
    "    os.makedirs(duplicates_folder, exist_ok=True)\n",
    "    unique_images = []\n",
    "    duplicate_images = []\n",
    "\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        is_duplicate = False\n",
    "        for unique_img_path in unique_images:\n",
    "            unique_img = cv2.imread(unique_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if orb_similarity(img, unique_img):\n",
    "                duplicate_images.append(img_path)\n",
    "                is_duplicate = True\n",
    "                break\n",
    "\n",
    "        if not is_duplicate:\n",
    "            unique_images.append(img_path)\n",
    "\n",
    "    # Move duplicate images to duplicates folder\n",
    "    for img_path in duplicate_images:\n",
    "        shutil.move(img_path, os.path.join(duplicates_folder, os.path.basename(img_path)))\n",
    "\n",
    "    print(f\"Moved {len(duplicate_images)} duplicate images to '{duplicates_folder}'.\")\n",
    "\n",
    "# Usage\n",
    "image_folder = r'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\ImagesofFace'\n",
    "duplicates_folder = r'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\Repeated Images'\n",
    "find_duplicates(image_folder, duplicates_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc05db9-922b-4069-9f41-852f2eb31f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imagehash\n",
      "  Using cached ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting PyWavelets (from imagehash)\n",
      "  Using cached pywavelets-1.7.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from imagehash) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\python312\\lib\\site-packages (from imagehash) (10.3.0)\n",
      "Collecting scipy (from imagehash)\n",
      "  Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Using cached ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "Using cached pywavelets-1.7.0-cp312-cp312-win_amd64.whl (4.2 MB)\n",
      "Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "Installing collected packages: scipy, PyWavelets, imagehash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Python312\\\\images'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\python312\\lib\\site-packages (10.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install imagehash\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f7c9d9-7bc7-4709-83ae-0e1f3601c709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 28 duplicate images to 'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\Repeated Images'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "def find_duplicates(image_folder, duplicates_folder, hash_size=8):\n",
    "    os.makedirs(duplicates_folder, exist_ok=True)\n",
    "    hashes = {}\n",
    "    duplicate_images = set()\n",
    "\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        with Image.open(img_path) as img:\n",
    "            img_hash = imagehash.phash(img, hash_size=hash_size)\n",
    "\n",
    "        # Check if the hash already exists in the dictionary\n",
    "        if img_hash in hashes:\n",
    "            duplicate_images.add(img_file)\n",
    "        else:\n",
    "            hashes[img_hash] = img_file\n",
    "\n",
    "    # Move duplicate images to duplicates folder\n",
    "    for img in duplicate_images:\n",
    "        shutil.move(os.path.join(image_folder, img), os.path.join(duplicates_folder, img))\n",
    "\n",
    "    print(f\"Moved {len(duplicate_images)} duplicate images to '{duplicates_folder}'.\")\n",
    "\n",
    "# Usage\n",
    "image_folder = r'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\ImagesofFace'\n",
    "duplicates_folder = r'D:\\HRL\\comprehensiv\\Data_Analysis_Comprehensiv\\Repeated Images'\n",
    "find_duplicates(image_folder, duplicates_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b2a59-e415-4c87-b0b3-86b54222854c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
